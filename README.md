# Visualize

Interpretability of Deep Learning Models with Tensorflow 2.0

## Activations Visualization

> Visualize how a given input comes out of a specific activation layer

<p align="center">
    <img src="./docs/assets/activations_visualisation.png" width="500" />
</p>


## Occlusion Sensitivity

> Visualize how parts of the image affects neural network's confidence

<p align="center">
    <img src="./docs/assets/occlusion_sensitivity.png" width="200" />
</p>


## Roadmap

- [x] Activations Visualization
- [ ] Convolutional Kernel Visualizations
- [x] Occlusion Sensitivity Maps
- [ ] Saliency Maps
- [ ] Grad-CAM
